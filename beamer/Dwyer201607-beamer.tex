%  pk-EFM theory paper
%  Created by Ryan P. Dwyer July 8, 2016

% %%   PREAMBLE   %%%

% PREPRINT
%\RequirePackage[displaymath,mathlines]{lineno}  % implement numbered lines
%\documentclass[aps,prl,preprint,citeautoscript,superscriptaddress,byrevtex,nofootinbib]{revtex4}
\documentclass[xcolor={dvipsnames}]{beamer}
\usepackage{presentation}
\usetheme{Boadilla}
\usecolortheme{beaver}
\setbeamertemplate{itemize items}[circle]


% GALLEY
%\RequirePackage[displaymath,mathlines]{lineno}  % implement numbered lines -- pagewise for two-column mode
% \documentclass[10pt,aps,prl,twocolumn,galley,citeautoscript,superscriptaddress,byrevtex,nofootinbib,nobalancelastpage,floatfix]{revtex4}

% TWO COLUMN
%\RequirePackage[displaymath,mathlines,pagewise]{lineno}  % implement numbered lines -- pagewise for two-column mode
%\RequirePackage[displaymath,mathlines]{lineno}  % implement numbered lines -- pagewise for two-column mode
% \documentclass[aps,prl,twocolumn,citeautoscript,superscriptaddress,byrevtex,nofootinbib,nobalancelastpage,floatfix]{revtex4}

% PACKAGES

\usepackage{siunitx}    % package for \meter etc
\usepackage{graphicx}         % \includegraphics{}
\usepackage{array}
\usepackage{mathtools}
\usepackage{amsmath,amsfonts,amssymb,amsthm} % For math equations, theorems, sy
\makeatletter
\def\zz\ignorespaces{\@ifnextchar-{}{\phantom{-}}}
\newcolumntype{C}{>{\zz}{c}}
\makeatother
\usepackage{bm}                       % need for bold greek letters
\usepackage{hyperref}
\hypersetup{colorlinks=true, urlcolor=blue}
\usepackage{mciteplus}
\usepackage{caption}
\captionsetup[figure]{labelformat=empty}% redefines the caption setup of the figures environment in the beamer class.
\usepackage{booktabs}
\captionsetup{font=scriptsize,labelfont=scriptsize}
\usepackage{natbib}  % bibliography
%\RequirePackage{lineno}
       


% FONTS

% Computer Modern is the default LaTeX font.
% Uncomment one of the lines below to try another font.
% PRL actually appears to use a font closet to Times.
% \usepackage{times}     % ~not~ computer modern fonts
\usepackage{palatino}  % ~not~ computer modern fonts

% FORMATTING OPTIONS

\lefthyphenmin=3           % Fix LaTeX hyphenation
\righthyphenmin=4          % Fix LaTeX hyphenation
%\setlength{\parskip}{6pt}  % Set paragraph spacing to be easy on the eyes

\graphicspath{
{/Users/ryandwyer/Dropbox/_JAM_MS__Dwyer201607__pk_EFM_theory__figs/}
}

% COMMANDS
\newcommand{\trimcaptionspacing}{\vspace{-0.25in}}      % include in figures to decrease the text-to-figure spacing
\newcommand{\trimcaptionspacinghalf}{\vspace{-0.10in}}  % include in figures to decrease the text-to-figure spacing
\def\bibfont{\footnotesize} % Smaller font in the bibliography

% Easy text subscripts
\newcommand{\st}[1]{_\text{#1}}

% See http://tex.stackexchange.com/questions/21598/how-to-color-math-symbols
\makeatletter
\def\mathcolor#1#{\@mathcolor{#1}}
\def\@mathcolor#1#2#3{%
  \protect\leavevmode
  \begingroup
    \color#1{#2}#3%
  \endgroup
}
\makeatother

 
 
 
%Information to be included in the title page:

\title[What is the SVD?]{What is the singular value decomposition?}

\author{Ryan Dwyer \inst{1}}

\institute[]{
    \inst{1} Department of Chemistry and Chemical Biology \\
    Cornell University}

\date[2016.11.16]{Marohn Group Meeting \\
November 16, 2016}

\begin{document}

\frame{\titlepage}

\begin{frame}
\frametitle{Applications of the singular value decomposition}
\begin{itemize}
    \item Linear least squares solution (pseudoinverse)
    \item Approximating matrices
    \item Optimization
    \item Recommender systems (Netflix, PageRank)
    \item Quantum information (Schmidt decomposition)
\end{itemize}
\end{frame}


% \item 
% 
% 
% \end{equation}

\begin{frame}
\frametitle{Matrix decompositions make certain operations easy}
\begin{exampleblock}{Analogy: Eigendecomposition}
\begin{align*}
\underbrace{A^n = A A \ldots = \; ? \tfrac{}{}}_\text{$n$ matrix multiplications: slow, obtuse}& & \underbrace{e^{A t} = I + A t + \tfrac{1}{2} A^2 t^2 + \ldots = \; ?}_\text{power series: potentially even worse!}
\end{align*}
\begin{align*}
A& = \underbrace{Q \Lambda Q^{-1}}_{\substack{\Lambda \: \text{diagonal}}} \Rightarrow & & \underbrace{A^n = Q \Lambda^n Q^{-1}, \quad e^{At} = Q e^{\Lambda} Q^{-1}}_\text{scalar functions: fast, understandable}
\end{align*}
\begin{itemize}
    \item Eigendecomposition diagonalizes square $n \times n$ matrices
    \item Matrix multiplication or power series $\Rightarrow$ scalar function evaluation
    \item Solution of differential and difference equations, Markov chains
\end{itemize}
\end{exampleblock}
\end{frame}

% \begin{frame}
% SVD compactly describes how a matrix $A$ transforms vectors $\mathbf{x}$ to vectors $\mathbf{y}$,
% \begin{equation}
% \mathbf{y} = A \mathbf{x}
% \end{equation}
% \end{frame}

\begin{frame}
\frametitle{Singular value decomposition}
Understand how $m \times n$ matrices $A$ transform $n$-dimensional $\mathbf{x}$ to $m$-dimensional $\mathbf{y}$.
\begin{equation*}
\mathbf{y} = A \mathbf{x}
\end{equation*}
\begin{itemize}
    \item How does a small change $\delta \mathbf{x}$ affect $\mathbf{y}$?
    \item How does a small change $\delta \mathbf{y}$ affect $\mathbf{x}$?
    \item Is $A$ singular or \emph{almost} singular?
    \item How well can $A$ be approximated / compressed?
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Problems that can be formulated as $\mathbf{y} = A \mathbf{x}$}
\begin{exampleblock}{Examples}
\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{ %
\begin{tabular}{ccccc}

Problem & $\mathbf{y}$ & $A$ & $\mathbf{x}$ \\ \midrule
\begin{minipage}[t]{1.75in}%
Ordinary least squares (e.g., line fit)%
\end{minipage} & 
Data, $\begin{pmatrix} y(t_1) \\ y(t_2) \\ \vdots \end{pmatrix}$& 
$\begin{pmatrix}1 & t_1 \\ 1 & t_2 \\ \vdots & \vdots \end{pmatrix}$ & $\begin{pmatrix} b \\ m \end{pmatrix}$, fit parameters &  \\
\begin{minipage}[t]{1.75in}%
Non-linear least squares (e.g., exponential fit)%
\end{minipage}&
Residuals, $\begin{pmatrix} y(t_1) - f(t_1, \mathbf{x}_0) \\ y(t_2) - f(t_2, \mathbf{x}_0) \\ \vdots \end{pmatrix}$& 
$\nabla f(\mathbf{x}_0)$ &
$\begin{pmatrix} \delta y_0 \\ \delta \tau \end{pmatrix}$, change in fit param.
\\
\begin{minipage}[t]{1.75in}%
Convolution,
$y = H \ast x$%
\end{minipage}
 & Output, $\begin{pmatrix} y(t_{k}) \\ y(t_{k+1}) \\ \vdots \end{pmatrix}$&%
\begin{minipage}{0.6in}%
\centering%
Toeplitz matrix for $H$%
\end{minipage} & Input, $\begin{pmatrix} x(t_1) \\ x(t_2) \\ \vdots \end{pmatrix}$
\end{tabular}
%
}
\end{table}
\end{exampleblock}
\pause
\begin{block}{}
\centering
Majority of scientists' data analysis and signal processing falls into one of these categories!
\end{block}
\end{frame}


\begin{frame}
\frametitle{Singular value decomposition}
\begin{itemize}
    \item SVD decomposes an $m \times n$ matrix A into,
\begin{equation}
A = U \Sigma V^T
\end{equation}
    \item $V$, $U$ orthonormal bases for $\mathbf{x}$, $\mathbf{y}$
    \item $\Sigma$ a diagonal matrix of \emph{positive} singular values
\end{itemize}
\begin{figure}
\includegraphics[width=2in]{figs/Singular-Value-Decomposition.png}
\caption{By Georg-Johann - Own work, CC BY-SA 3.0, \href{https://commons.wikimedia.org/w/index.php?curid=11342212}{https://commons.wikimedia.org/w/index.php?curid=11342212}}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{What is the matrix $A$?}
\begin{columns}
\column{0.58 \textwidth}
Fit data points to a parabola,
$$f(t) = c_0 + c_1 t + c_2 t^2.$$
Since $f$ is \emph{linear} in each parameter $c_i$,
the output $y$ of the function at a collection of times $t_1, t_2, \ldots t_M$ as,
$$y = A x$$.
\pause
\column{0.4 \textwidth}
\vspace{-0.5in}
\begin{figure}
\includegraphics[height=0.86\textheight]{figs/A.pdf}
\end{figure}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{SVD: picture}
\vspace{-0.3in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/svd-components.pdf}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{SVD: picture}
\vspace{-0.1in}
\begin{figure}
\includegraphics[width=0.7\textwidth]{figs/svd-components.pdf}
\end{figure}
$$y = A x = U \Sigma V^T x$$
\vspace{-0.2in}
\begin{exampleblock}{}
$V^T$ changes bases for the space of $x$ (rotation).

$\Sigma$ scales each basis vector $v_i$ by $\sigma_i$.

$U$ transforms the scaled $v_i$ (in the space of $x$) to the space of $y$.
\end{exampleblock}
\end{frame}

% \begin{frame}
% \frametitle{Action of a matrix}
% \begin{figure}
% \includegraphics[width=3.75in]{figs/Strang1993nov-fig1.png}
% \end{figure}
% \end{frame}


\begin{frame}
\frametitle{Example: Exponential decay curve fitting}
\begin{columns}
\onslide<1->{
    \column{0.3\textwidth}
        \begin{alertblock}{}
        How does changing parameters affect the best-fit result?
        \end{alertblock}
        \begin{alertblock}{}
        How does changing data affect the parameter values?
        \end{alertblock}
}
\onslide<2->{
    \begin{block}{}
    $\delta y_0$ affects $y$ near $t=0$.
    \end{block}
}
\onslide<1->{
    \column{0.68\textwidth}
    \begin{figure}
        \includegraphics[width=\textwidth]{figs/dA.pdf}
    \end{figure}
}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Example: Exponential decay curve fitting}
\begin{columns}
\onslide<1->{
    \column{0.3\textwidth}
        \begin{alertblock}{}
        How does changing parameters affect the best-fit result?
        \end{alertblock}
        \begin{alertblock}{}
        How does changing data affect the parameter values?
        \end{alertblock}
}
\onslide<2->{
    \begin{exampleblock}{}
    $\delta \tau$ affects $y$ near $t=\tau_0$
    \end{exampleblock}
}
\onslide<1->{
    \column{0.68\textwidth}
    \begin{figure}
        \includegraphics[width=\textwidth]{figs/dtau.pdf}
    \end{figure}
}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Linearize the equation about a point}
\begin{columns}
    \column{0.25\textwidth}
        \begin{align*}
        \mathbf{y}& = f(\mathbf{x}) \\
        \delta \mathbf{y}& \approx \nabla f(\mathbf{x}_0) \delta \mathbf{x} \\
        J& = \nabla f(\mathbf{x}_0) \\
        \mathbf{x}& = (y_0 \; \; \tau)^T
        \end{align*}

    \column{0.75\textwidth}
        \begin{figure}
        \includegraphics[width=\textwidth]{figs/dy0dtau.pdf}
        \end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{How does $y$ affect the parameters?}
\begin{figure}
\includegraphics[width=0.95\textheight]{figs/perturb-y.pdf}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Compare:}
\begin{columns}
    \column{0.45\textwidth}
    \begin{figure}
        \includegraphics[width=\textwidth]{figs/dy0dtau.pdf}
    \end{figure}
    \column{0.45\textwidth}
    \begin{figure}
        \includegraphics[width=\textwidth]{figs/perturb-y.pdf}
    \end{figure}
\end{columns}
These relationships are complicated and hard to describe because $(y_0 \; \; \tau)^T$
and $\left( f(t_0) \; \; f(t_1) \; \; f(t_2) \right)^T$ are poor bases to describe the \emph{action} of the matrix $J$.
\end{frame}

\begin{frame}
\frametitle{Singular value decomposition}
\begin{columns}
\hspace{-0.35in}
\column{0.7\textwidth}
\vspace{-0.36in}
\begin{figure}
\includegraphics[height=0.86\textheight]{figs/dy-usv.pdf}
\end{figure}
\column{0.25\textwidth}
$$\delta y = U \Sigma V^T \delta x$$
$$\delta x = V \Sigma^{+} U^T \delta y$$
$V$: basis for parameters

$\Sigma$: scale basis vectors

$U$: basis for $\delta y(t)$

\vspace{0.2in}

Each component $u_i \sigma_i v_i$ can be analyzed individually.
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Extension: double exponential curve fit}
\begin{block}{Problem}
Double exponential fitting is ``ill-conditioned.'' Why?
\end{block}
\begin{figure}
\includegraphics[height=0.7\textheight]{figs/two-time-constants.pdf}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Extension: double exponential curve fit}
\begin{columns}
    \column{0.25\textwidth}
    \begin{align*}
    \mathbf{y}& = f(\mathbf{x}) \\
    \delta \mathbf{y}& \approx \nabla f(\mathbf{x}_0) \delta \mathbf{x} \\
    J& = \nabla f(\mathbf{x}_0) \\
    \mathbf{x}& = \begin{pmatrix}
    y_1 \\
    y_2 \\
    \tau_1 \\
    \tau_2
    \end{pmatrix}
    \end{align*}

    \column{0.75\textwidth}
    \begin{figure}
    \includegraphics[width=\textwidth]{figs/two-time-constants-Jac.pdf}
    \end{figure}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{SVD of double exponential}
\vspace{-0.25in}
\begin{columns}
    \column{0.58\textwidth}
    \begin{figure}
    \includegraphics[width=\textwidth]{figs/double-expon-svd.pdf}
    \end{figure}
    \vspace{-0.3in}
    \begin{block}{}
    99.7\% energy in first 3 singular values.

    Jacobian $J$ is ``almost'' singular.

    Data $y$ gives almost no information on second time constant $\tau_2$.
    \end{block}

    \column{0.4\textwidth}
    \vspace{-0.35in}
    \begin{figure}
    \includegraphics[width=1.07\textwidth]{figs/two-time-constants-Jac.pdf}
    \end{figure}
    \begin{equation*}
    \begin{matrix*}[C]
    & v_1 & v_2 & v_3 & v_4 \\
\delta y_1& -2& 4& 9& 2 \\
\delta y_2& -8& -5& 0& -3 \\
\delta \tau_1& -5& 7& -5& 2 \\
\delta \tau_2& -1& -4& -1& \mathcolor{purple}{9} \\
    \end{matrix*}
    \end{equation*}



\end{columns}
\end{frame}

\begin{frame}
\frametitle{Optimization applications: least squares}
\begin{itemize}
    \item Consider data $y$ fit to $A x$, where $x$ is a vector of parameters
    \item Define vector of residuals $r$,
    $$r = y - A x$$
    \item Objective: minimize square of residuals
\begin{align*}
\min_{x} f(x)& = \frac{1}{2} \lVert r \rVert^2 = \frac{1}{2} r^T r 
\end{align*}
\end{itemize}
\begin{block}{What is special about least squares problems?}
Residuals give ``free'' information about derivatives
\end{block}
\end{frame}

\begin{frame}
\frametitle{Optimization applications: least squares}
\begin{exampleblock}{Gradient}
By the chain rule,
\begin{align*}
 \nabla f = - A^T r,
\end{align*}
Residuals projected back to the space of $x$ with $A^T$ is the steepest descent direction.
\end{exampleblock}
\begin{exampleblock}{Hessian}
$$\nabla^2 f = A^T A $$

In non-linear least squares, Jacobian of residuals $J$ gives a good guess for the Hessian.
\end{exampleblock}
\end{frame}

\begin{frame}
\frametitle{Best fit line, minimizing $f = \frac{1}{2} r^T r$}
\begin{figure}
\includegraphics[width=0.93\textwidth]{figs/linear_fit.pdf}
\end{figure}
\begin{block}{}
Curvature of $f$ is $\frac{1}{2}A^T A$.

Eigenvectors of $A^T A$ are the vectors $v_i$ from the SVD.

Eigenvalues of $A^T A$ are $\sigma_i^2$.
\end{block}
\end{frame}

\begin{frame}
\frametitle{Optimization applications: Landweber iteration}
Choose the next value of the parameter vector $x$ using,
\begin{align*}
x_{k+1}& = x_k + \alpha \, A^T r_k 
\end{align*}
\begin{block}{}
This is gradient descent with a fixed step size, using the residuals $r$ to compute the gradient direction (since $\nabla f(x_k) = - A^T r_k$)
\end{block}
\end{frame}

\begin{frame}
\frametitle{Stability of Landweber iteration}
Write the iteration in terms of the least-squares solution $x^+$, and the error $e_k$,
\begin{align*}
e_k & = x^+ - x_k \hspace{0.25in} \Rightarrow \hspace{0.25in} x_k = x^+ - e_k
\end{align*}
Write the recursion in terms of the error:
\begin{align*}
x_{k+1}& = x^+ - e_{k+1} = x_k + \alpha \, A^T r_k \\
x_{k+1}& = x^+ - e_{k+1} = x^+ - e_k + \alpha \, A^T A e_k \\
e_{k+1}& = (I - \alpha A^T A) e_k.
\intertext{Write $A^T A$ using the SVD,}
e_{k+1}& = (\underbrace{I}_{VV^T} - \alpha V \Sigma \underbrace{U^T U}_{I} \Sigma V^T) e_k\\
e_{k+1}& = V(I - \alpha \Sigma^2)V^T e_k \\
V^T e_{k+1}& = (I - \alpha \Sigma^2)V^T e_k
\end{align*}
\end{frame}

% \begin{align*}
% \intertext{Write $A^T A$ using the SVD,}
% e_{k+1}& = (\underbrace{I}_{VV^T} - \alpha V \Sigma \underbrace{U^T U}_{I} \Sigma V^T) e_k\\
% e_{k+1}& = V(I - \alpha \Sigma^2)V^T e_k \\
% V^T e_{k+1}& = (I - \alpha \Sigma^2)V^T e_k
% \end{align*}



\begin{frame}
\frametitle{Stability of Landweber iteration}
Define the SVD basis with hats, $\hat{x} = V^T x$:
\begin{align*}
V^T e_{k+1}& = (I - \alpha \Sigma^2)V^T e_k \\
\hat{e}_{k+1}& = (I - \alpha \Sigma^2) \hat{e}_k.
\intertext{The explicit solution is}
\hat{e}_k& = (\underbrace{I - \alpha \Sigma^2}_{\text{diagonal}})^k \hat{e}_0.
\intertext{This is stable for $\alpha < 2/\sigma_1^2$. For an initial guess of $\mathbf{0}$, the initial error $e_0 = x^+$}
\hat{e}_k& = (I - \alpha \Sigma^2)^k \hat{x}^+
\intertext{The $k^{\text{th}}$ solution vector is}
\hat{x}_k& = (I - (I - \alpha \Sigma^2)^k) \hat{x}^+
\end{align*}
\end{frame}

\begin{frame}
\frametitle{Different regularized solutions}
\begin{columns}
\column{0.3\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/sv-spectrum.pdf}
\end{figure}
\column{0.68\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/regularization.pdf}
\end{figure}
\end{columns}
\begin{exampleblock}{}
Regularized solutions can be written in terms of effect on SVD components.
\end{exampleblock}
\end{frame}

\begin{frame}
\frametitle{Image compression}
\begin{columns}
\column{0.5\textwidth}
\hspace{-0.25in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill.png}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill-1.png}
\end{figure}
\end{columns}
Use truncated SVD for image compression, keeping largest $N$ singular values.
\end{frame}

\begin{frame}
\frametitle{Image compression}
\begin{columns}
\column{0.5\textwidth}
\hspace{-0.25in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill.png}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill-3.png}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Image compression}
\begin{columns}
\column{0.5\textwidth}
\hspace{-0.25in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill.png}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill-10.png}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Image compression}
\begin{columns}
\column{0.5\textwidth}
\hspace{-0.25in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill.png}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill-30.png}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Image compression}
\begin{columns}
\column{0.5\textwidth}
\hspace{-0.25in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill.png}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill-50.png}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Image compression}
\begin{columns}
\column{0.5\textwidth}
\hspace{-0.25in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill.png}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/mandrill-100.png}
\end{figure}
\end{columns}
\end{frame}


\begin{frame}
\frametitle{Image reconstruction}
\begin{columns}
\column{0.5\textwidth}
\hspace{-0.25in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/noise-mandrill.png}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/noise-mandrill-3.png}
\end{figure}
\end{columns}
Use truncated SVD for image reconstruction, keeping largest $N$ singular values.
\end{frame}

\begin{frame}
\frametitle{Image reconstruction}
\begin{columns}
\column{0.5\textwidth}
\hspace{-0.25in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/noise-mandrill.png}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/noise-mandrill-10.png}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Image reconstruction}
\begin{columns}
\column{0.5\textwidth}
\hspace{-0.25in}
\begin{figure}
\includegraphics[width=\textwidth]{figs/noise-mandrill.png}
\end{figure}
\column{0.5\textwidth}
\begin{figure}
\includegraphics[width=\textwidth]{figs/noise-mandrill-20.png}
\end{figure}
\end{columns}
\end{frame}

\begin{frame}
\frametitle{Conclusions}
\begin{block}{SVD gives information about}
\begin{itemize}
    \item How does a small change $\delta \mathbf{x}$ affect $\mathbf{y}$?
    \item How does a small change $\delta \mathbf{y}$ affect $\mathbf{x}$?
    \item Is $A$ singular or \emph{almost} singular?
    \item How well can $A$ be approximated / compressed?
\end{itemize}
\end{block}
\begin{alertblock}{}
Really useful for statistics, data analysis, etc.
\end{alertblock}
\end{frame}

% \end{frame}

% \begin{frame}
% \frametitle{Examples}
% \begin{figure}
% \includegraphics[width=3in]{figs/Strang1993nov-fig4.png}
% \end{figure}
% Linear least squares:
% \begin{equation}
% y = \begin{pmatrix}
% -1 & 1 \\
% 0 & 1 \\
% 1 & 1 \\
% \end{pmatrix}
% \begin{pmatrix}
% \alpha \\
% \beta
% \end{pmatrix}
% \end{equation}
% \end{frame}

% \begin{frame}
% \frametitle{Examples}
% \begin{figure}
% \includegraphics[width=3in]{figs/Strang1993nov-fig4.png}
% \end{figure}
% Linear least squares:
% \begin{equation}
% A = \begin{pmatrix}
% -1/\sqrt{3} & -1/\sqrt{2} \\
% -1/\sqrt{3} & 0 \\
% -1/\sqrt{3} & 1/\sqrt{2}
% \end{pmatrix}
% \begin{pmatrix}
%     \sqrt{3}
% \end{pmatrix}
% \begin{pmatrix}
% \alpha \\
% \beta
% \end{pmatrix}
% \end{equation}
% \end{frame}



% =========================
% \def\bibsection{\vspace{6pt}}
% \setlength{\bibsep}{0pt}

% \bibliographystyle{bst/naturemag_jm}

% \bibliography{bib/jam99_2012-03-29_Ryan}
% \label{TheEnd}
\end{document}
